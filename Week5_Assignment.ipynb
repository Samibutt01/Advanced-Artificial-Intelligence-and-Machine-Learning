{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "88c9e264",
      "metadata": {
        "id": "88c9e264"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mouryarahul/7CS107_PracticalWorks_Assignment/blob/master/Week5_Assignment.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49937e8d",
      "metadata": {
        "id": "49937e8d"
      },
      "source": [
        "# 7CS107: Advanced AI and Machine Learning — Programming Assignment (Week 5)\n",
        "\n",
        "**Topic coverage:** Decision Theory, Naive Bayes, Logistic Regression, Support Vector Machines (SVM), model evaluation.\n",
        "\n",
        "**Total marks:** 100 (5 questions × 20 marks)\n",
        "\n",
        "**Allowed libraries:** NumPy, SciPy, scikit-learn, pandas, matplotlib/seaborn, and Python standard library.\n",
        "\n",
        "**Submission:** Submit this **executed** notebook (.ipynb) on the Canvas with **all outputs visible**.\n",
        "\n",
        "> **Academic integrity:** Work must be your own. Cite any external sources. You may discuss general ideas, but code must be written independently.\n",
        "\n",
        "---\n",
        "\n",
        "## How to work through this assignment (step-by-step)\n",
        "\n",
        "1. **Read the task** in the markdown cell before each question.\n",
        "2. **Open the code cell** that contains a function with a `# TODO` block.\n",
        "3. **Implement only inside `# TODO`** (do not change function names or signatures).\n",
        "4. **Run the code cell** to define your function.\n",
        "5. **Run the test cell** that follows. The test cell:\n",
        "   - Creates or loads data,\n",
        "   - Calls your function,\n",
        "   - Computes metrics,\n",
        "   - Checks thresholds using `assert` statements.\n",
        "6. If a test fails:\n",
        "   - Read the **inline comments** (every line in the test cells is commented to explain intent),\n",
        "   - Print intermediate values if needed,\n",
        "   - Refine your code and re-run.\n",
        "7. **Keep code concise** (a few lines are sufficient). You may add brief comments to explain your reasoning.\n",
        "8. **Save and re-run `Kernel > Restart & Run All`** before submission to ensure a clean run.\n",
        "\n",
        "### Marking scheme (per question, 20 marks)\n",
        "- **Implementation correctness (12 marks):** passes tests; follows the requested approach.\n",
        "- **Result quality (5 marks):** metrics meet/beat thresholds in the tests (or are well-justified if borderline).\n",
        "- **Code quality (3 marks):** clear, concise, readable; uses appropriate library functions.\n",
        "\n",
        "### Datasets used (auto-downloaded by scikit-learn)\n",
        "- **20 Newsgroups (text)**: `sklearn.datasets.fetch_20newsgroups`\n",
        "- **Breast Cancer Wisconsin**: `sklearn.datasets.load_breast_cancer`\n",
        "- **Iris**: `sklearn.datasets.load_iris`\n",
        "\n",
        "For dataset documentation, see scikit-learn docs (no manual download needed).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a600094a",
      "metadata": {
        "id": "a600094a"
      },
      "outputs": [],
      "source": [
        "# Standard imports used across questions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups, load_breast_cancer, load_iris\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, f1_score, accuracy_score,\n",
        "                             roc_auc_score, roc_curve)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Global random seed for reproducibility across tests\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf0bd9a0",
      "metadata": {
        "id": "cf0bd9a0"
      },
      "source": [
        "## Q1 (20 marks) — Decision Theory: Bayes Optimal (Cost-Sensitive) Classifier for Gaussian Models\n",
        "\n",
        "We consider a two-class problem with **known** class-conditional densities and class priors:\n",
        "- $x\\mid y=k \\sim \\mathcal{N}(\\mu_k,\\Sigma_k)$,\n",
        "- Priors $(\\pi_0,\\pi_1)$,\n",
        "- Optional **cost matrix** $C$ where $C[i,j]$ is the cost of predicting class $j$ when the true class is $i$.\n",
        "\n",
        "### Your task\n",
        "1. Compute the **posterior** $P(y=k\\mid x)$ using Bayes' rule. Use **log-densities** for numerical stability.\n",
        "2. If a cost matrix is supplied, choose the class that **minimizes expected risk**:\n",
        "   $$\\hat{y}(x)=\\arg\\min_j \\sum_i C[i,j] \\cdot P(y=i\\mid x).$$\n",
        "   If `cost_matrix=None`, perform **MAP** (argmax posterior).\n",
        "\n",
        "### Implementation tips\n",
        "- Use `scipy.stats.multivariate_normal.logpdf` for $\\log p(x\\mid y=k)$.\n",
        "- Convert log-posteriors to probabilities with a log-sum-exp trick: subtract the row-wise max, exponentiate, and normalize.\n",
        "- Return a 1D array of predicted class indices of shape `(n,)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "308c0a27",
      "metadata": {
        "id": "308c0a27"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence, Optional\n",
        "\n",
        "def bayes_decision_gaussian(X: np.ndarray,\n",
        "                            priors: Sequence[float],\n",
        "                            mus: Sequence[np.ndarray],\n",
        "                            covs: Sequence[np.ndarray],\n",
        "                            cost_matrix: Optional[np.ndarray] = None) -> np.ndarray:\n",
        "    \"\"\"Bayes (cost-sensitive) classifier for Gaussian class-conditionals.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : (n, d) array of inputs.\n",
        "    priors : length-K sequence of class priors that sum to 1.\n",
        "    mus : list of K arrays of shape (d,) (class means).\n",
        "    covs : list of K arrays of shape (d,d) (class covariances).\n",
        "    cost_matrix : (K,K) array or None. If None, do MAP; else minimize expected risk.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y_pred : (n,) array of predicted class indices (0..K-1)\n",
        "    \"\"\"\n",
        "    X = np.asarray(X)\n",
        "    K = len(priors)\n",
        "    n = X.shape[0]\n",
        "\n",
        "    # TODO: Compute unnormalized log-posteriors: log p(x|y=k) + log pi_k\n",
        "    log_posts = np.zeros((n, K))\n",
        "\n",
        "    # TODO: Normalize to get posteriors using log-sum-exp stabilization\n",
        "    # subtract row-wise max\n",
        "    # exponentiate stabilized values\n",
        "    # row-wise normalization to 1\n",
        "\n",
        "    if cost_matrix is None:\n",
        "        # return MAP decision\n",
        "\n",
        "    # TODO: Expected risk for predicting class j: sum_i C[i,j] * P(y=i|x)\n",
        "    # compute expected risks for all classes\n",
        "    # return minimize expected risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd28d66c",
      "metadata": {
        "id": "cd28d66c",
        "outputId": "aaff37a8-e6a3-42e0-c9d5-48f74691c470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion (cost-sensitive): [[74 16]\n",
            " [ 1 29]]\n",
            "Confusion (MAP): [[86  4]\n",
            " [ 5 25]]\n",
            "Empirical expected risk (cost-sensitive): 0.175\n",
            "Empirical expected risk (MAP):           0.242\n",
            "[Q1] Tests passed ✅\n"
          ]
        }
      ],
      "source": [
        "# === Tests for Q1 ===\n",
        "np.random.seed(0)  # set seed for this test to make it reproducible\n",
        "\n",
        "# --- Create a synthetic 2D Gaussian dataset for two classes ---\n",
        "mu0 = np.array([0.0, 0.0])                # mean of class 0\n",
        "mu1 = np.array([2.0, 2.0])                # mean of class 1\n",
        "Sigma0 = np.array([[1.0, 0.2],            # covariance of class 0\n",
        "                   [0.2, 1.0]])\n",
        "Sigma1 = np.array([[1.0, -0.3],           # covariance of class 1\n",
        "                   [-0.3, 1.2]])\n",
        "priors = [0.6, 0.4]                       # class priors p(y=0)=0.6, p(y=1)=0.4\n",
        "\n",
        "# Sample points from each class\n",
        "n0, n1 = 90, 30                          # number of samples per class\n",
        "X0 = np.random.multivariate_normal(mu0, Sigma0, size=n0)  # samples of class 0\n",
        "X1 = np.random.multivariate_normal(mu1, Sigma1, size=n1)  # samples of class 1\n",
        "X = np.vstack([X0, X1])                    # stack into a single dataset (n, d)\n",
        "y_true = np.hstack([np.zeros(n0, dtype=int), np.ones(n1, dtype=int)])  # true labels (n,)\n",
        "\n",
        "# --- Define a cost matrix to penalize false negatives more heavily ---\n",
        "# cost[i,j] = cost of predicting j when true class is i\n",
        "cost = np.array([[0.0, 1.0],               # predicting 1 when true is 0 costs 1\n",
        "                 [5.0, 0.0]])              # predicting 0 when true is 1 costs 5 (more serious)\n",
        "\n",
        "# --- Call the student's function in two modes: cost-sensitive and MAP ---\n",
        "y_pred_cost = bayes_decision_gaussian(X, priors, [mu0, mu1], [Sigma0, Sigma1], cost_matrix=cost)  # minimize expected risk\n",
        "y_pred_map  = bayes_decision_gaussian(X, priors, [mu0, mu1], [Sigma0, Sigma1], cost_matrix=None)  # MAP (no costs)\n",
        "\n",
        "# --- Compute confusion matrices to see error patterns ---\n",
        "conf_cost = confusion_matrix(y_true, y_pred_cost, labels=[0, 1])  # confusion for cost-sensitive predictions\n",
        "conf_map  = confusion_matrix(y_true, y_pred_map,  labels=[0, 1])  # confusion for MAP predictions\n",
        "\n",
        "# --- Compute empirical expected risk for each strategy ---\n",
        "# Multiply elementwise by the cost matrix and average by number of samples\n",
        "risk_cost = (conf_cost * cost).sum() / len(X)\n",
        "risk_map  = (conf_map  * cost).sum() / len(X)\n",
        "\n",
        "# --- Display diagnostics ---\n",
        "print(\"Confusion (cost-sensitive):\", conf_cost)\n",
        "print(\"Confusion (MAP):\", conf_map)\n",
        "print(f\"Empirical expected risk (cost-sensitive): {risk_cost:.3f}\")\n",
        "print(f\"Empirical expected risk (MAP):           {risk_map:.3f}\")\n",
        "\n",
        "# --- Assertion: cost-sensitive decision should not have higher expected risk than MAP here ---\n",
        "assert risk_cost <= risk_map + 1e-6, \"Cost-sensitive decision should not yield higher expected risk than MAP here.\"\n",
        "print(\"[Q1] Tests passed ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25092cc8",
      "metadata": {
        "id": "25092cc8"
      },
      "source": [
        "## Q2 (20 marks) — Naive Bayes on Text (20 Newsgroups)\n",
        "\n",
        "Train a **Multinomial Naive Bayes** classifier on a **2-class** subset of the 20 Newsgroups dataset (default: `sci.space` vs `rec.autos`).\n",
        "\n",
        "### Your task\n",
        "1. Load train **and** test splits for the chosen categories with `fetch_20newsgroups` (remove headers/footers/quotes).\n",
        "2. Vectorize text using `CountVectorizer(min_df=2)` (bag-of-words counts).\n",
        "3. Fit `MultinomialNB(alpha=alpha)` on training data and predict on test data.\n",
        "4. Return **macro F1** on the test set.\n",
        "\n",
        "### Tips\n",
        "- Keep code compact (4–6 lines inside the function is enough).\n",
        "- Use provided defaults: `categories=(\"sci.space\",\"rec.autos\")`, `alpha=1.0`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "eab1e73e",
      "metadata": {
        "id": "eab1e73e"
      },
      "outputs": [],
      "source": [
        "def train_nb_20ng(categories=(\"sci.space\", \"rec.autos\"), alpha: float = 1.0) -> float:\n",
        "    \"\"\"Train MultinomialNB on a two-class 20NG subset and return macro F1 on the test set.\"\"\"\n",
        "    # TODO: load train & test data for the specified categories\n",
        "    data_train = fetch_20newsgroups(subset='train', categories=list(categories), remove=('headers', 'footers', 'quotes'))\n",
        "    data_test  = fetch_20newsgroups(subset='test',  categories=list(categories), remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "    # TODO: vectorize text and fit NB\n",
        "    # Use CountVectorizer with min_df=2 to vectorize the text data\n",
        "    # Fit and transform the training data\n",
        "    # Transform the test data\n",
        "    vectorizer = CountVectorizer(min_df=2)\n",
        "    X_train = vectorizer.fit_transform(data_train.data)\n",
        "    X_test  = vectorizer.transform(data_test.data)\n",
        "    # create Multinomial Naive Bayes classifier with given alpha\n",
        "    # fit the model\n",
        "    # predict on test data\n",
        "    clf = MultinomialNB(alpha=alpha)\n",
        "    clf.fit(X_train, data_train.target)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    # TODO: compute macro F1\n",
        "    # return macro F1 score\n",
        "    return f1_score(data_test.target, y_pred, average='macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "acd7fb62",
      "metadata": {
        "id": "acd7fb62",
        "outputId": "25586a4e-9304-4ce7-c0f6-dbf8cbe5cd9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Macro F1: 0.898\n",
            "[Q2] Tests passed ✅\n"
          ]
        }
      ],
      "source": [
        "# === Tests for Q2 ===\n",
        "# Train on the default categories and evaluate macro F1\n",
        "f1 = train_nb_20ng()                                     # run train_nb_20ng function with defaults\n",
        "print(f\"Macro F1: {f1:.3f}\")                             # display the macro-averaged F1 score\n",
        "\n",
        "# Minimal performance threshold for this simple baseline\n",
        "assert f1 >= 0.75, \"F1 should be at least 0.75 on this binary subset with bag-of-words + NB.\"\n",
        "print(\"[Q2] Tests passed ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e060c13d",
      "metadata": {
        "id": "e060c13d"
      },
      "source": [
        "## Q3 (20 marks) — Logistic Regression (Breast Cancer) with ROC–AUC\n",
        "\n",
        "Train a **Logistic Regression** classifier on the Breast Cancer Wisconsin dataset. Use a train/test split (stratified), a standardization step, and report **ROC–AUC** on the test set.\n",
        "\n",
        "### Your task\n",
        "1. Split into train/test with `train_test_split(..., stratify=y, test_size=0.25, random_state=42)`.\n",
        "2. Build a pipeline: `StandardScaler()` → `LogisticRegression(max_iter=1000, solver='liblinear')`.\n",
        "3. Return **ROC–AUC** on the test set using predicted probabilities.\n",
        "\n",
        "### Tips\n",
        "- The `solver='liblinear'` works well for smaller datasets; keep defaults unless you experiment.\n",
        "- Expose `C` and `class_weight` as parameters to the function and pass them into the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "89fa2154",
      "metadata": {
        "id": "89fa2154"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "def logreg_breast_cancer(C: float = 1.0, class_weight=None) -> float:\n",
        "    \"\"\"Train LogisticRegression on breast cancer dataset and return ROC–AUC on test set.\"\"\"\n",
        "    # TODO: load data and split\n",
        "    # load breast cancer dataset\n",
        "    # split into train and test sets\n",
        "    data = load_breast_cancer()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.30, random_state=42, stratify=y\n",
        "    )\n",
        "    # TODO: pipeline and fit\n",
        "    # create pipeline with scaling and logistic regression\n",
        "    # fit the model\n",
        "    scaler = StandardScaler()\n",
        "    X_train_s = scaler.fit_transform(X_train)\n",
        "    X_test_s = scaler.transform(X_test)\n",
        "\n",
        "    clf = LogisticRegression(C=C, class_weight=class_weight, solver='liblinear', max_iter=1000, random_state=42)\n",
        "    clf.fit(X_train_s, y_train)\n",
        "    # TODO: ROC–AUC on test set\n",
        "    # probability of positive class\n",
        "    #  return ROC-AUC score\n",
        "    y_prob = clf.predict_proba(X_test_s)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_prob)\n",
        "    return auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "af9fb42e",
      "metadata": {
        "id": "af9fb42e",
        "outputId": "0a7b2f20-716d-420e-8a67-900902b46c57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test ROC–AUC: 0.998\n",
            "[Q3] Tests passed ✅\n"
          ]
        }
      ],
      "source": [
        "# === Tests for Q3 ===\n",
        "auc = logreg_breast_cancer()                          # call logreg_breast_cancer with defaults\n",
        "print(f\"Test ROC–AUC: {auc:.3f}\")                     # display the ROC–AUC on the test split\n",
        "\n",
        "# Require a strong baseline with proper scaling (typical performance is high on this dataset)\n",
        "assert auc >= 0.95, \"ROC–AUC should be at least 0.95 on Breast Cancer with scaling + LR.\"\n",
        "print(\"[Q3] Tests passed ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0492b42e",
      "metadata": {
        "id": "0492b42e"
      },
      "source": [
        "## Q4 (20 marks) — RBF SVM (Iris) with Cross-Validation\n",
        "\n",
        "Train an **RBF-kernel SVM** on the Iris dataset using a pipeline (scaling + SVC). Report **mean accuracy** using stratified 5-fold cross-validation.\n",
        "\n",
        "### Your task\n",
        "1. Build pipeline: `StandardScaler()` → `SVC(kernel='rbf', C=C, gamma=gamma)`.\n",
        "2. Compute **mean accuracy** via stratified K-fold CV (default `cv=5`, shuffled with `random_state=42`).\n",
        "3. Return the **mean** CV accuracy.\n",
        "\n",
        "### Tips\n",
        "- Use `cross_val_score` with `StratifiedKFold` to ensure class balance across folds.\n",
        "- Keep defaults unless you want to experiment with `C` and `gamma`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "39176a74",
      "metadata": {
        "id": "39176a74"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "def svm_rbf_iris(C: float = 1.0, gamma='scale', cv: int = 5) -> float:\n",
        "    \"\"\"Train SVC with RBF kernel on Iris and return mean CV accuracy.\"\"\"\n",
        "    # TODO: load data and split\n",
        "    # create pipeline with scaling and SVC\n",
        "    # perform stratified CV\n",
        "    # return mean accuracy\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "\n",
        "    model = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=C, gamma=gamma, random_state=42))\n",
        "\n",
        "    cv_splitter = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
        "    scores = cross_val_score(model, X, y, cv=cv_splitter, scoring='accuracy')\n",
        "\n",
        "    return scores.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fd07833f",
      "metadata": {
        "id": "fd07833f",
        "outputId": "38736c85-d790-424d-e9d8-51d3ccce2116",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean CV accuracy: 0.960\n",
            "[Q4] Tests passed ✅\n"
          ]
        }
      ],
      "source": [
        "# === Tests for Q4 (fully commented) ===\n",
        "acc = svm_rbf_iris()                              # run svm_rbf_iris with defaults\n",
        "print(f\"Mean CV accuracy: {acc:.3f}\")             # display mean accuracy across folds\n",
        "\n",
        "# The Iris dataset is clean and low-dimensional; RBF SVM typically achieves high accuracy\n",
        "assert acc >= 0.95, \"Mean CV accuracy should be at least 0.95 on Iris with RBF SVM.\"\n",
        "print(\"[Q4] Tests passed ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0e271e8",
      "metadata": {
        "id": "c0e271e8"
      },
      "source": [
        "## Q5 (20 marks) — Nested Cross-Validation for SVM (Breast Cancer)\n",
        "\n",
        "Perform **nested cross-validation** to estimate the generalization performance of an RBF SVM on the Breast Cancer dataset while tuning hyperparameters $(C, \\gamma)$.\n",
        "\n",
        "### Your task\n",
        "1. **Outer loop**: `StratifiedKFold(outer_k, shuffle=True, random_state=42)` splits data into train/test.\n",
        "2. **Inner loop**: `GridSearchCV` over a parameter grid for `C` and `gamma` with `StratifiedKFold(inner_k, shuffle=True, random_state=123)`.\n",
        "3. For each outer split, fit the inner **grid search** on the outer-train portion, then evaluate the **best model** on the outer-test portion.\n",
        "4. Return `(mean_acc, std_acc)` across all outer folds.\n",
        "\n",
        "### Tips\n",
        "- Build pipeline: `StandardScaler()` → `SVC(kernel='rbf')` and name parameters in grid as `svc__C`, `svc__gamma`.\n",
        "- Keep grids small for runtime: defaults `(0.1, 1, 10)` for `C`, and `('scale', 0.01, 0.1, 1.0)` for `gamma`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6799c3db",
      "metadata": {
        "id": "6799c3db"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "def nested_cv_svm_breast(outer_k: int = 3, inner_k: int = 3,\n",
        "                         Cs = (0.1, 1, 10), gammas = ('scale', 0.01, 0.1, 1.0)):\n",
        "    \"Perform nested CV with SVM on breast cancer and return mean and std of outer accuracies.\"\"\"\n",
        "    # TODO: load data\n",
        "    ds = load_breast_cancer()\n",
        "    X, y = ds.data, ds.target\n",
        "\n",
        "    #TODO: perform outer CV (stratified)\n",
        "    outer_cv = StratifiedKFold(n_splits=outer_k, shuffle=True, random_state=42)\n",
        "    outer_scores = []\n",
        "\n",
        "    # TODO: for each outer fold, perform inner CV grid search and evaluate on outer test fold\n",
        "    for train_idx, test_idx in outer_cv.split(X, y):\n",
        "        # Split data for this outer fold\n",
        "        X_tr, X_te = X[train_idx], X[test_idx]\n",
        "        y_tr, y_te = y[train_idx], y[test_idx]\n",
        "\n",
        "        # Perform Inner CV grid search on the training portion of the outer fold\n",
        "        model = make_pipeline(StandardScaler(), SVC(kernel='rbf', random_state=42))\n",
        "        param_grid = {'svc__C': Cs, 'svc__gamma': gammas}\n",
        "        inner_cv = StratifiedKFold(n_splits=inner_k, shuffle=True, random_state=42)\n",
        "        grid = GridSearchCV(model, param_grid, cv=inner_cv, scoring='accuracy')\n",
        "        grid.fit(X_tr, y_tr)\n",
        "        # Evaluate the best model found in inner loop on the held-out outer test split\n",
        "        best_model = grid.best_estimator_\n",
        "        y_pred = best_model.predict(X_te)\n",
        "        outer_scores.append(accuracy_score(y_te, y_pred))\n",
        "\n",
        "    # return mean and std of outer accuracies\n",
        "    return {np.mean(outer_scores), np.std(outer_scores)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4dbfdc6a",
      "metadata": {
        "id": "4dbfdc6a",
        "outputId": "66f39f27-f923-4f00-8362-07f810129e50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outer mean accuracy: 0.975 ± 0.013\n",
            "[Q5] Tests passed ✅\n"
          ]
        }
      ],
      "source": [
        "# === Tests for Q5 ===\n",
        "# Run nested CV with default outer/inner folds and parameter grids\n",
        "mean_acc, std_acc = nested_cv_svm_breast()                      # returns mean and std accuracy across outer folds\n",
        "print(f\"Outer mean accuracy: {mean_acc:.3f} ± {std_acc:.3f}\")   # display aggregate performance\n",
        "\n",
        "# Expect strong performance on Breast Cancer with proper scaling and model selection\n",
        "assert mean_acc >= 0.93, \"Outer mean accuracy should be at least 0.93 for SVM on Breast Cancer.\"\n",
        "print(\"[Q5] Tests passed ✅\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XpormLpT2d2-"
      },
      "id": "XpormLpT2d2-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}